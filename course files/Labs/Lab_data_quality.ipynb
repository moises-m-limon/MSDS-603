{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality\n",
    "\n",
    "Checking the quality of the data used in your analysis is a key step, and often done during an EDA process. But, it can easily be overlooked when moving to production, and even sometimes overlooked by the data engineering team. If there is any doubt that the data your application will be ingesting may have quality control (QC) issues, you should be empowered to set up your own QC process yourself.\n",
    "\n",
    "It is amazing how quickly and easily data can go from being good, high-quality to being junk. A seemingly innocuous change in the schema of one single upstream table can ruin all downstream tables and applications. It is important to remember that, especially in large organizations, there may be multiple teams with ownership of multiple sources of data, and not all of these teams will have disciplined and knowledgeable engineers working on them. \n",
    "\n",
    "The most important data sources to test for quality will typically be the raw sources of data as it comes in, but you can test whichever data sources you have access to since you may not have certain privileges. You may even feel inclined to create tests further downstream to ensure that whatever preprocessing, feature generation, model scoring, and postprocessing code is written is resulting in *expected* data types, shapes, sizes, values and distributions. \n",
    "\n",
    "## Great Expectations\n",
    "\n",
    "[Great Expectations](https://greatexpectations.io/) is a popular, free, and mature data quality tool which allows you to easiy write tests, or *expectations*, using python. From the website, you can use it to validate, document and profile your data. Great Expecations does NOT do data versioning or orchestrate data pipelines.\n",
    "\n",
    "The high-level steps for using Great Expectations are: (a) install Great Expectations; (b) create and configure a Data Context; (c) create your expectations (tests); and (d) validate your data. However, let's start simpler and use the great expectations python library.\n",
    "\n",
    "Let's install Great Expectations, within our virtual environment, with \n",
    "\n",
    "`pip install great_expectations`  \n",
    "\n",
    "and add it to our requirements.txt file. From here, check out the help files:\n",
    "\n",
    "`great_expectations --help`  \n",
    "\n",
    "You'll see that, thankfully, there are only a handful of commands, though each command has many optional arguments, but we' won't worry about these right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import great_expectations as ge\n",
    "import pandas as pd\n",
    "\n",
    "# First, I'm going to add a header to my data and resave it. It is annoying not having a header.\n",
    "col_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'y']\n",
    "path = \"../data/\"\n",
    "train = pd.read_csv(f\"{path}adult.data\", names = col_names)\n",
    "test = pd.read_csv(f\"{path}adult.test\", names = col_names)\n",
    "\n",
    "train.to_csv(f\"{path}adult_train.csv\", index = False)\n",
    "test.to_csv(f\"{path}adult_test.csv\", index = False)\n",
    "\n",
    "# Now reload the new data and make sure it looks right\n",
    "train = pd.read_csv(f\"{path}adult_train.csv\")\n",
    "test = pd.read_csv(f\"{path}adult_test.csv\")\n",
    "\n",
    "# Here we are creating two datasets that we can use with Great Expectations\n",
    "train_df = ge.dataset.PandasDataset(train)\n",
    "test_df = ge.dataset.PandasDataset(test)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table-Level Expectations\n",
    "\n",
    "First, let's think of what we expect our entire table to look like. \n",
    "\n",
    "What columns should be there? \n",
    "Do any columns form a unique identifier?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns\n",
    "train_df.expect_table_columns_to_match_ordered_list(\n",
    "    column_list=['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'y']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column list to make success = false\n",
    "test_df.expect_table_columns_to_match_ordered_list(\n",
    "    column_list=['Rage', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'y']\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column-Level Expectations\n",
    "\n",
    "Second, let's think about our individual columns. \n",
    "\n",
    "- What data types should they be?   \n",
    "- Should there be any missing values?  \n",
    "- What are the minimum and maximum values?  \n",
    "- etc.\n",
    "\n",
    "There are many more expectations which you can find [here](https://greatexpectations.io/expectations), and you can customize your own expectations.\n",
    "\n",
    "Go left to right and define expectations for each column. We'll just do our first two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age\n",
    "## should be integer\n",
    "train_df.expect_column_values_to_be_of_type(column=\"age\", type_=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.expect_column_values_to_be_of_type(column=\"age\", type_=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Should have no missing values\n",
    "train_df.expect_column_values_to_not_be_null(column=\"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.expect_column_values_to_not_be_null(column=\"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workclass\n",
    "## should be string\n",
    "train_df.expect_column_values_to_be_of_type(column=\"workclass\", type_=\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## should have 9 unique values\n",
    "train_df.expect_column_unique_value_count_to_be_between(column=\"workclass\", min_value=9, max_value=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## should have 9 unique values\n",
    "test_df.expect_column_unique_value_count_to_be_between(column=\"workclass\", min_value=9, max_value=9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation Suite\n",
    "\n",
    "After defining the expectations we can collect them all into a suite and then use the validate method to run them all and show us any expectations that fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expectation suite\n",
    "expectation_suite = train_df.get_expectation_suite(discard_failed_expectations=False)\n",
    "print(train_df.validate(expectation_suite=expectation_suite, only_return_failures=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expectation_suite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Great Expectations Within a Project\n",
    "\n",
    "Next, let's use Great Expectations using the CLI. This way, we can set up our testing suites and document them along with our results. First, we need to initialize and configure our Data Context using \n",
    "\n",
    "`great_expectations init`\n",
    "\n",
    "The context is how we will then interact with great expectations, and you will see it appear again later. Running this command will create a great_expectations.yml file which will be our main configuration file. We can add this to our git repo, and ignore the rest. Notice there is a new great_expectations folder, and inside that folder is another folder called uncommitted. The 'uncommitted' folder is meant to be ignored by git, and you'll notice has already been added to a .gitignore file in the great_expectations folder.\n",
    "\n",
    "There is some valuable information in the great_expectations.yml file we should take a look at before moving on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a New Data Source\n",
    "\n",
    "Next, we should tell Great Expectations about our data sources. We can use data from our local filesystem, or data stored in a relational database. We also must tell Great Expectations how we are processing our data, either using pandas or spark. Let's set up our data sources by running:\n",
    "\n",
    "`great_expectations datasource new`\n",
    "\n",
    "In the terminal, we will be asked:  \n",
    "```\n",
    "What data would you like Great Expectations to connect to?  \n",
    "    1. Files on a filesystem (for processing with Pandas or Spark)  \n",
    "    2. Relational database (SQL)  \n",
    "```\n",
    "\n",
    "Select 1 for filesystem. Then we will be asked:  \n",
    "\n",
    "```\n",
    "What are you processing your files with?  \n",
    "1. Pandas  \n",
    "2. PySpark  \n",
    "```\n",
    "\n",
    "Select 1 for Pandas. Lastly, we will be asked:  \n",
    "\n",
    "```\n",
    "Enter the path of the root directory where the data files are stored: \n",
    "```\n",
    "\n",
    "We should enter the path as `data`. A Jupyter notebook will open up automatically which will allow us to configure our data source. We might only need to make one simple change to the notebook, to the `datasource_name` field (we can rename it to whatever makes the most sense). From here, we run each cell in the notebook, and then we can look in the great_expectations.yml file to see that our new data source was added to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a New Suite\n",
    "\n",
    "Next, let's create a suite of expectations by running:\n",
    "\n",
    "`great_expectations suite new`\n",
    "\n",
    "In the terminal we will be asked: \n",
    "\n",
    "```\n",
    "How would you like to create your Expectation Suite?  \n",
    "    1. Manually, without interacting with a sample batch of data (default)  \n",
    "    2. Interactively, with a sample batch of data  \n",
    "    3. Automatically, using a profiler  \n",
    "```\n",
    "\n",
    "Select 2 so that a sample of data will be used to test our expectations. Then we will be asked:\n",
    "\n",
    "```\n",
    "Which data asset (accessible by data connector \"default_inferred_data_connector_name\") would you like to use?  \n",
    "```\n",
    "\n",
    "We will select whichever dataset we want to create a suite for. Let's choose adult_train.csv. And lastly, we will be asked to name the suite. After naming the suite, a notebook will open and we can write our expectations directly in the notebook, and run the cells to validate the expectations. \n",
    "\n",
    "Notice that the suite will be saved in the expectations folder as a json file. If we ever need to check which suites we've created, or edit any of our suites, we can simply run \n",
    "\n",
    "`great_expectations suite list`  \n",
    "`great_expectations suite edit suite_name`  \n",
    "\n",
    "One of the nice things about Great Expectations is that it makes it easy to document our expectations and results. We can build and launch our docs in a browser by running:\n",
    "\n",
    "`great_expectations docs build`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Profiler\n",
    "\n",
    "Rather than manually create all of our expectations ourselves, we can use a User Configerable Profiler to do some of the work for us. A profiler will create a set of very *strict* expectations, expectations that *overfit* our data and go beyond what we would probably want. After the expectation suite is created using the profiler, we can review the expectations and make any adjustments, additions or deletions.\n",
    "\n",
    "We can use the profiler in one of two ways: (1) the CLI; or (2) a .py file. Let's use the CLI by running \n",
    "\n",
    "`great_expectations suite new --profile`\n",
    "\n",
    "We'll be asked to select a data asset for the suite and then name the suite. Let's pick adult_train.csv and name the suite \"adult_train_data\". Great Expectations will launch a notebook that we can then edit. First we should comment out any columns that we don't want **excluded** from the profiler. After that we can run each code cell in the notebook in order to create the expectation suite. \n",
    "\n",
    "After using the profiler, and reviewing the expectations that were created, we should run `great_expectations edit suite adult_train_data` to edit the expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoints\n",
    "\n",
    "Checkpoints are used to validate data. Suppose you want to run a checkpoint every time a data pipeline job runs, you can run a checkpoint. We can see what checkpoints exist by running \n",
    "\n",
    "`great_expectations checkpoint list`\n",
    "\n",
    "and create a new one by running \n",
    "\n",
    "`great_expectations checkpoint new {NAME}`\n",
    "\n",
    "This will open up a notebook that you can then edit to configure a checkpoint. Be sure to check that the yaml for the checkpoint contains the expectation suite that you want to use for the checkpoint and the correct data set.\n",
    "\n",
    "After creating a checkpoint you can run it in the command line:\n",
    "\n",
    "`great_expectations checkpoint run {NAME}`  \n",
    "\n",
    "Or you can run a checkpoint within a python script like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ge.get_context()\n",
    "checkpoint = context.get_checkpoint(\"data_ingestion\")\n",
    "checkpoint.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a checkpoint is run, it would make sense to take some action based on the results, such as sending an email or a Slack notification if there are failures (or even if they all run successfully). This can, theoretically, be done within Great Expectations itself by defining a new `action` within the yaml file for the checkpoint that was created above (which you can find in the `checkpoints/` folder). There is an action called EmailAction for sending emails, or SlackNotificationAction for sending Slack notifications. Each action requires some configurations, and possible changes to your `config_variables.yml` file found under the `uncommited/` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Great Expectations Summary\n",
    "\n",
    "There is a lot that Great Expectations can do, and we've only scratched the surface, but hopefully you've noticed how *simple* it can be to create, validate, and document tests for your data without having to write your own unit tests and corresponding application. We have not yet shown how to include Great Expectations in your data pipelines - there are integrations with several popular orchestration engines such as Airflow and Prefect. We'll cover these in a future session.\n",
    "\n",
    "Most of what we've done has been from the CLI which launches notebooks that we have to run and edit - it's all very interactive (and admittedly feels a little strange). There are more pythonic ways of using Great Expectations by simply using the library in some python scripts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Tools \n",
    "\n",
    "Admittedly, Great Expectations is a heavy tool. It takes a lot of setup to do things. But, notice what was done in each step:\n",
    "\n",
    "- Define the data sources  \n",
    "- Create the table-level and column-level expectations (tests)  \n",
    "- Combine expectations for a data source into a suite  \n",
    "- Create a checkpoint, with actions  \n",
    "- Visualize results in a report or dashboard  \n",
    "\n",
    "Each of these steps can also be accomplished with other tools. We can create unit tests, using `pytest`, and then using a scheduler we can schedule those tests to run every time we ingest new data, or on a schedule, and output the results to a log file, and email the results or send a Slack notification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Lab\n",
    "\n",
    "## Overview \n",
    "\n",
    "In this lab we will learn about running tests for data quality for our model data.\n",
    "\n",
    "## Goal\n",
    "\n",
    "The goal of this lab is to set up *unit tests* for your project data.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "### Great Expectations \n",
    "\n",
    "There are a few tools out there for data quality, but we will be using Great Expectations since it is one of the most popular. Deepchecks is another newer tool that checks more than just the data, it also checks model outputs.\n",
    "\n",
    "Remember the high-level steps for using Great Expectations are: (a) install Great Expectations; (b) create and configure a Data Context; (c) create your expectations (tests); and (d) validate your data.\n",
    "\n",
    "Before beginning, be aware of the data set that you will be checking for quality. In a real job scenario, you should absolutely have checks for every column, but since I am not paying you, do not spend hours and hours doing this for this assignment. If your data has more than ten columns, then you can choose the ten **most important** features and create expectations just for those five, and then use a profiler for the rest. \n",
    "\n",
    "#### Install Great Expectations, Create and Configure a Data Context\n",
    "\n",
    "1. Install Great Expectations (in your virtual environment): `pip install great_expectations`. You can also add it to your requirements.txt file. I'm using version 0.18.19.   \n",
    "2. Create a Data Context by initializing Great Expectations: `great_expectations init`  \n",
    "3. Add everything to git. The `uncommited/` folder should automatically not be added.  \n",
    "4. Look over the great_expectations.yml file.  \n",
    "5. Create a data source: `great_expectations datasource new`  \n",
    "\n",
    "#### Create Expectations\n",
    "\n",
    "1. Create a new suite of expectations for your data sets: `great_expectations suite new`.    \n",
    "    - If you have more than ten columns, create expectations only for the **top ten** columns.  \n",
    "\n",
    "#### Validate Data\n",
    "\n",
    "1. Run the cells in the notebook that contains your suite of expectations.  \n",
    "    - Fix any data issues that are revealed by your suite.    \n",
    "2. Take a look at the docs: `great_expectations docs build`.  \n",
    "\n",
    "#### Profile Data\n",
    "\n",
    "1. Create a new suite of expectations for your data sets using a User Configurable Profiler.  \n",
    "    - Edit this suite down to only the essential expectations needed for you data. \n",
    "    - **Do not** simply use the expectations that are given. Most of these will not make sense and are just a starting point. \n",
    "2. You should now have two suites of expectations for your data. At this point you can combine expectations into one suite for each set of data.  \n",
    "\n",
    "#### Create a Checkpoint  \n",
    "\n",
    "1. Create a new checkpoint. This is essentially what you can run every time you need to check the data, or run it on a schedule.  \n",
    "2. Run the pipeline.  \n",
    "3. Build the docs.  \n",
    "\n",
    "### Turning It In\n",
    "\n",
    "Make sure you push to Github and submit your Github URL in Canvas. Your repo should contain the yaml files for your initialized Great Expectations data context and checkpoint and the json files for your expectation suite. Also, please zip up your data docs folder and upload this to Canvas. \n",
    "\n",
    "### Grade\n",
    "\n",
    "This lab is worth 10 points. Each step above should be completed. There should be **meaningful** tests for your project data table and columns, and the checkpoint should have been run a few times and should appear in the data docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4097c64d94c5b44aa96706a47b357d91e727b3cbacdc6414ea202e6f06a3d71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
